{"cells":[{"metadata":{"trusted":false},"id":"wrong-strand","cell_type":"code","source":"#Importing libraries\nimport numpy as np \nimport pandas as pd \nimport io\nimport matplotlib.pyplot as plt","execution_count":2,"outputs":[]},{"metadata":{"trusted":false},"id":"olive-potential","cell_type":"code","source":"# Load properties and data\ndata = pd.read_csv('BuyComputer.csv')\n\ndata.drop(columns=['User ID',],axis=1,inplace=True)\ndata.head()","execution_count":4,"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>EstimatedSalary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>19000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35</td>\n      <td>20000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>43000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>57000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>76000</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Age  EstimatedSalary  Purchased\n0   19            19000          0\n1   35            20000          0\n2   26            43000          0\n3   27            57000          0\n4   19            76000          0"},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"id":"hourly-watson","cell_type":"code","source":"#Declare label as last column in the source file\nLabels = data['Purchased']\nLabels\n#Declaring X as all columns excluding last\nX = data.iloc[:,:2]\n# Splitting data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Labels,random_state=142,test_size=0.3)","execution_count":9,"outputs":[]},{"metadata":{"trusted":false},"id":"latter-mapping","cell_type":"code","source":"# Sacaling data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":10,"outputs":[]},{"metadata":{"trusted":false},"id":"affiliated-insider","cell_type":"code","source":"X_test.mean(axis=0)","execution_count":11,"outputs":[{"data":{"text/plain":"array([-8.69674703e-17, -1.22124533e-16])"},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"id":"golden-rapid","cell_type":"code","source":"class LogisticRegression: \n    __sigmoid = lambda self,z :1/(1+pow(np.e,-z))\n    __loss_func = lambda self,Y,Y_pred : -((Y)@(np.log(Y_pred).T) + (1-Y)@(np.log(1-Y_pred).T))\n    __Wgrad = lambda self,Y,Y_pred,X : (Y_pred-Y)@X\n    __bgrad = lambda self,Y,Y_pred : (Y_pred-Y).sum()\n    \n    W,b = None,None\n    def __init__(self):\n        pass\n\n    def fit(self,X,Y,epoch=100,alpha = 1e-3,lmd=1):\n        self.W = np.random.randn(X.shape[1])\n        self.b = np.random.random()\n        \n        \n        for e in range(epoch):\n            pred = self.predict(X)\n            cost = self.__loss_func(Y,pred)\n            print(f\"{e+1} : cost = {cost}\")\n            self.W-=alpha*(self.__Wgrad(Y,pred,X)+lmd*self.W)\n            self.b-=alpha*self.__bgrad(Y,pred)\n#             print(self.W,self.b)\n        return self\n            \n    \n    def predict(self,X_test):\n        if self.W is None:\n            raise Exception(\"model is not fitted yet!\")\n#         print(\"------------>\",self.W.shape,self.b,\"<-------------------\")\n        Z = X_test@(self.W.T)+self.b\n        predictions = self.__sigmoid(Z)\n        return predictions","execution_count":12,"outputs":[]},{"metadata":{"trusted":false},"id":"latest-vulnerability","cell_type":"code","source":"np.random.random()","execution_count":13,"outputs":[{"data":{"text/plain":"0.3811631384775881"},"execution_count":13,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"id":"minimal-control","cell_type":"code","source":"mymodel = LogisticRegression()\nmymodel.fit(X_train,y_train,alpha=1e-2,epoch=60)","execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"1 : cost = 335.0001262693166\n2 : cost = 159.69969092767212\n3 : cost = 118.81701231991812\n4 : cost = 107.14191771959852\n5 : cost = 102.27123722898511\n6 : cost = 99.63872368173642\n7 : cost = 97.9896507451803\n8 : cost = 96.86364452576274\n9 : cost = 96.05191894999064\n10 : cost = 95.44468131619692\n11 : cost = 94.97794698533276\n12 : cost = 94.61162414254404\n13 : cost = 94.31923272317823\n14 : cost = 94.08257415993356\n15 : cost = 93.8887467980709\n16 : cost = 93.72837190547463\n17 : cost = 93.59448845531521\n18 : cost = 93.4818380277823\n19 : cost = 93.38638753545814\n20 : cost = 93.30500228260561\n21 : cost = 93.235216981581\n22 : cost = 93.17507226744108\n23 : cost = 93.12299599521708\n24 : cost = 93.07771575958166\n25 : cost = 93.03819356161608\n26 : cost = 93.00357642894201\n27 : cost = 92.97315868766034\n28 : cost = 92.94635285141263\n29 : cost = 92.92266695602979\n30 : cost = 92.90168676568625\n31 : cost = 92.88306169601702\n32 : cost = 92.8664935981663\n33 : cost = 92.85172776272225\n34 : cost = 92.83854565905798\n35 : cost = 92.82675904080561\n36 : cost = 92.81620513377402\n37 : cost = 92.80674268677026\n38 : cost = 92.79824871426268\n39 : cost = 92.79061579674574\n40 : cost = 92.78374983298738\n41 : cost = 92.77756816021248\n42 : cost = 92.77199797527206\n43 : cost = 92.76697500313732\n44 : cost = 92.76244236950197\n45 : cost = 92.75834964253116\n46 : cost = 92.7546520153511\n47 : cost = 92.75130960610323\n48 : cost = 92.74828685657843\n49 : cost = 92.74555201382012\n50 : cost = 92.743076681811\n51 : cost = 92.74083543256947\n52 : cost = 92.73880546778082\n53 : cost = 92.73696632356106\n54 : cost = 92.73529961215483\n55 : cost = 92.73378879536313\n56 : cost = 92.73241898531364\n57 : cost = 92.73117676886596\n58 : cost = 92.73005005250508\n59 : cost = 92.72902792504794\n60 : cost = 92.72810053587867\n"},{"data":{"text/plain":"<__main__.LogisticRegression at 0x7f28703da2b0>"},"execution_count":15,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"id":"aquatic-spider","cell_type":"code","source":"y_pred = mymodel.predict(X_test)\ny_pred ","execution_count":16,"outputs":[{"data":{"text/plain":"array([0.99359549, 0.38846445, 0.06284155, 0.86843877, 0.38060547,\n       0.36109615, 0.0029131 , 0.75885893, 0.75668201, 0.00758778,\n       0.93577758, 0.09294752, 0.00654511, 0.31075683, 0.04251526,\n       0.54362012, 0.58024943, 0.08042952, 0.38452743, 0.4999903 ,\n       0.06067666, 0.01113028, 0.55624468, 0.33156995, 0.34043502,\n       0.00586895, 0.94673345, 0.00547442, 0.00932198, 0.04574135,\n       0.11311529, 0.95435869, 0.63518249, 0.5035482 , 0.06326193,\n       0.05706307, 0.00156581, 0.01298481, 0.39737692, 0.13784161,\n       0.50295523, 0.88811966, 0.17255549, 0.14577976, 0.11311529,\n       0.43920195, 0.7831355 , 0.99798365, 0.96225125, 0.87291272,\n       0.28448485, 0.19522987, 0.26360767, 0.77434727, 0.00960552,\n       0.01012688, 0.00224548, 0.08557425, 0.49480025, 0.0254938 ,\n       0.04261192, 0.08797113, 0.17221708, 0.00318141, 0.01063241,\n       0.97575532, 0.35360495, 0.44154019, 0.59605135, 0.99185159,\n       0.45989362, 0.98834791, 0.00177944, 0.00531217, 0.38452743,\n       0.15440279, 0.93972325, 0.01436773, 0.20078646, 0.03421026,\n       0.37142019, 0.14205314, 0.82987424, 0.91502054, 0.0430742 ,\n       0.27789171, 0.71706472, 0.65016322, 0.41915407, 0.62272563,\n       0.03307276, 0.03397593, 0.98325583, 0.62828188, 0.4478383 ,\n       0.97956739, 0.04184447, 0.38790111, 0.97699979, 0.4715512 ,\n       0.01359919, 0.20723894, 0.99068272, 0.91353336, 0.04386318,\n       0.00414568, 0.40007956, 0.24858228, 0.29117134, 0.93746711,\n       0.36548542, 0.00771384, 0.29695817, 0.00780061, 0.04251526,\n       0.84104111, 0.61503163, 0.82987424, 0.02349651, 0.01278113])"},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":true},"id":"korean-conviction","cell_type":"code","source":"#Using sklearn LogisticRegression model\n\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(random_state = 142)\n\n#Fit\nLR.fit(X_train,y_train)\n#predicting the test label with LR. Predict always takes X as input\ny1 = LR.predict(X_test)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"id":"wound-indian","cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y1))\nprint(classification_report(y_test,y_pred>=0.6))","execution_count":19,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.86      0.86      0.86        80\n           1       0.72      0.72      0.73        40\n\n    accuracy                           0.82       120\n   macro avg       0.79      0.79      0.79       120\nweighted avg       0.82      0.82      0.82       120\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.90      0.86        80\n           1       0.76      0.62      0.68        40\n\n    accuracy                           0.81       120\n   macro avg       0.79      0.76      0.77       120\nweighted avg       0.80      0.81      0.80       120\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}